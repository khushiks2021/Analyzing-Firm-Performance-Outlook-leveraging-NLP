{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the text <br>\n",
    "word_tokenize: tokenizes the text <br>\n",
    "3rd line of the function is to remove the stopwords <br>\n",
    "4th line of the function is to remove the punctuations <br>\n",
    "5th line of the function is to remove the numbers <br>\n",
    "6th line of the function is to remove the words with length less than 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "# Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    tokens = [re.sub(r'[^\\w\\s]', '', token) for token in tokens]\n",
    "    tokens = [token for token in tokens if not re.match(r'\\d+', token)]\n",
    "    tokens = [token for token in tokens if token.strip()]\n",
    "    return tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the TF-DIF matrix of the first document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TIFDIF(text):\n",
    "    # Compute TF-IDF\n",
    "    tokens = preprocess(text)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform([' '.join(tokens)])\n",
    "\n",
    "    # Access the computed TF-IDF scores\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    data = []\n",
    "    for col in tfidf_matrix.nonzero()[1]:\n",
    "        data.append({'Token': feature_names[col], 'TF-IDF score': tfidf_matrix[0, col]})\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Token  TF-IDF score\n",
      "0         peacock      0.001140\n",
      "1          golden      0.001140\n",
      "2         fishing      0.001140\n",
      "3            stop      0.001140\n",
      "4      mbillionth      0.001140\n",
      "...           ...           ...\n",
      "5129  performance      0.070710\n",
      "5130    financial      0.118611\n",
      "5131     analysis      0.070710\n",
      "5132   discussion      0.066149\n",
      "5133   management      0.177917\n",
      "\n",
      "[5134 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chetanyabhan/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "with open('prased_text.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "TIFDIF(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec Model of the first document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('inception', 0.3413967788219452), ('innovators', 0.3367283046245575), ('station', 0.32373788952827454), ('announced', 0.31935378909111023), ('multinationals', 0.3163487911224365), ('meet', 0.31118568778038025), ('Harnessing', 0.3046962022781372), ('JioThings', 0.3034692108631134), ('tracked', 0.2983028292655945), ('securing', 0.2831447124481201)]\n"
     ]
    }
   ],
   "source": [
    "tokens = preprocess(text)\n",
    "model = Word2Vec([tokens], min_count=1)\n",
    "word_vectors = model.wv\n",
    "print(model.wv.most_similar('support'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
